"""
üìä PERFORMANCE BENCHMARKER - SISTEMA ODONTOL√ìGICO
==============================================

Sistema avanzado de benchmarking de performance y m√©tricas para el m√≥dulo
odontol√≥gico. Realiza mediciones detalladas, comparaciones de performance
y genera reportes completos de optimizaci√≥n.

CARACTER√çSTICAS:
- Benchmarking autom√°tico de operaciones cr√≠ticas
- Comparaci√≥n de performance antes/despu√©s
- M√©tricas de UI rendering y responsiveness
- Profiling de computed variables
- Memory leaks detection
- Database query performance
- Real-time performance monitoring
- Reportes de optimizaci√≥n con recomendaciones

INTEGRACI√ìN: Todo el sistema odontol√≥gico + Performance Optimizer
"""

import reflex as rx
from typing import Dict, Any, List, Optional, Callable, Tuple
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass, field
import time
import asyncio
import statistics
import json
import random

from dental_system.state.app_state import AppState
from dental_system.models import (
    PacienteModel, ConsultaModel, IntervencionModel,
    ServicioModel, OdontogramaModel
)

# ==========================================
# üéØ TIPOS Y ENUMS PARA BENCHMARKING
# ==========================================

class BenchmarkType(Enum):
    """Tipos de benchmark"""
    OPERATION_TIMING = "operation_timing"
    UI_RENDERING = "ui_rendering"
    DATABASE_QUERY = "database_query"
    COMPUTED_VAR = "computed_var"
    MEMORY_USAGE = "memory_usage"
    USER_INTERACTION = "user_interaction"

class PerformanceLevel(Enum):
    """Niveles de performance"""
    EXCELLENT = "excellent"    # < 100ms
    GOOD = "good"             # 100-300ms
    ACCEPTABLE = "acceptable"  # 300-1000ms
    SLOW = "slow"             # 1000-3000ms
    UNACCEPTABLE = "unacceptable"  # > 3000ms

@dataclass
class BenchmarkResult:
    """Resultado de un benchmark individual"""
    benchmark_id: str
    operation_name: str
    benchmark_type: BenchmarkType
    execution_time_ms: float
    memory_before_mb: float
    memory_after_mb: float
    memory_delta_mb: float
    timestamp: datetime
    context: Dict[str, Any]
    performance_level: PerformanceLevel
    error: Optional[str] = None
    
    @property
    def performance_score(self) -> float:
        """Score de performance (0-100)"""
        if self.execution_time_ms <= 100:
            return 100.0
        elif self.execution_time_ms <= 300:
            return 90.0 - (self.execution_time_ms - 100) * 0.2
        elif self.execution_time_ms <= 1000:
            return 70.0 - (self.execution_time_ms - 300) * 0.1
        elif self.execution_time_ms <= 3000:
            return 40.0 - (self.execution_time_ms - 1000) * 0.02
        else:
            return max(0.0, 10.0 - (self.execution_time_ms - 3000) * 0.001)

@dataclass
class BenchmarkSuite:
    """Suite de benchmarks para una categor√≠a"""
    suite_name: str
    benchmarks: List[BenchmarkResult]
    total_execution_time: float
    average_performance_score: float
    slowest_operation: Optional[BenchmarkResult]
    fastest_operation: Optional[BenchmarkResult]
    memory_efficiency_score: float
    
    @classmethod
    def from_results(cls, suite_name: str, results: List[BenchmarkResult]):
        """Crear suite desde resultados"""
        if not results:
            return cls(
                suite_name=suite_name,
                benchmarks=[],
                total_execution_time=0.0,
                average_performance_score=0.0,
                slowest_operation=None,
                fastest_operation=None,
                memory_efficiency_score=100.0
            )
        
        total_time = sum(r.execution_time_ms for r in results)
        avg_score = statistics.mean([r.performance_score for r in results])
        slowest = max(results, key=lambda r: r.execution_time_ms)
        fastest = min(results, key=lambda r: r.execution_time_ms)
        
        # Calcular memory efficiency
        memory_leaks = [r for r in results if r.memory_delta_mb > 0]
        memory_efficiency = max(0, 100 - len(memory_leaks) * 10)
        
        return cls(
            suite_name=suite_name,
            benchmarks=results,
            total_execution_time=total_time,
            average_performance_score=avg_score,
            slowest_operation=slowest,
            fastest_operation=fastest,
            memory_efficiency_score=memory_efficiency
        )

@dataclass
class PerformanceComparison:
    """Comparaci√≥n de performance entre dos per√≠odos"""
    baseline_suite: BenchmarkSuite
    current_suite: BenchmarkSuite
    improvement_percentage: float
    regression_count: int
    improvement_count: int
    recommendations: List[str]

# ==========================================
# üìä ESTADO DEL PERFORMANCE BENCHMARKER
# ==========================================

class EstadoPerformanceBenchmarker(rx.State):
    """
    üìä Estado especializado en benchmarking de performance
    """
    
    # ==========================================
    # üéØ CONTROL DE BENCHMARKING
    # ==========================================
    
    # Estado del benchmarker
    benchmarking_active: bool = False
    current_benchmark: str = ""
    benchmark_progress: float = 0.0
    total_benchmark_suites: int = 6
    completed_suites: int = 0
    
    # Configuraci√≥n
    auto_benchmark_enabled: bool = True
    benchmark_interval_minutes: int = 30
    detailed_profiling: bool = False
    memory_profiling: bool = True
    
    # ==========================================
    # üìã RESULTADOS DE BENCHMARKING
    # ==========================================
    
    # Resultados actuales
    current_benchmark_results: List[BenchmarkResult] = []
    benchmark_suites: Dict[str, BenchmarkSuite] = {}
    
    # Historial de benchmarks
    benchmark_history: List[BenchmarkSuite] = []
    performance_timeline: List[Tuple[datetime, float]] = []
    
    # Comparaciones
    baseline_results: Optional[BenchmarkSuite] = None
    performance_comparisons: List[PerformanceComparison] = []
    
    # ==========================================
    # üìä M√âTRICAS AGREGADAS
    # ==========================================
    
    # Performance general
    overall_performance_score: float = 95.0
    performance_trend: str = "stable"  # improving, stable, degrading
    critical_slowdowns: List[str] = []
    
    # M√©tricas por categor√≠a
    ui_rendering_score: float = 90.0
    database_performance_score: float = 85.0
    memory_efficiency_score: float = 92.0
    computed_vars_score: float = 88.0
    
    # Alertas de performance
    performance_alerts: List[Dict[str, Any]] = []
    regression_alerts: List[str] = []
    
    # ==========================================
    # üí° COMPUTED VARS PARA AN√ÅLISIS
    # ==========================================
    
    @rx.var(cache=True)
    def benchmark_summary(self) -> Dict[str, Any]:
        """Resumen general de benchmarks"""
        if not self.benchmark_suites:
            return {
                "total_suites": 0,
                "avg_score": 0.0,
                "slowest_operation": "N/A",
                "fastest_operation": "N/A",
                "total_operations": 0
            }
        
        all_benchmarks = []
        for suite in self.benchmark_suites.values():
            all_benchmarks.extend(suite.benchmarks)
        
        if not all_benchmarks:
            return {"total_suites": len(self.benchmark_suites), "avg_score": 0.0}
        
        avg_score = statistics.mean([b.performance_score for b in all_benchmarks])
        slowest = max(all_benchmarks, key=lambda b: b.execution_time_ms)
        fastest = min(all_benchmarks, key=lambda b: b.execution_time_ms)
        
        return {
            "total_suites": len(self.benchmark_suites),
            "avg_score": avg_score,
            "slowest_operation": f"{slowest.operation_name} ({slowest.execution_time_ms:.1f}ms)",
            "fastest_operation": f"{fastest.operation_name} ({fastest.execution_time_ms:.1f}ms)",
            "total_operations": len(all_benchmarks)
        }
    
    @rx.var(cache=True)
    def performance_recommendations(self) -> List[str]:
        """Recomendaciones de performance"""
        recommendations = []
        
        # An√°lizar resultados para generar recomendaciones
        if self.database_performance_score < 80:
            recommendations.append("üóÑÔ∏è Optimizar consultas de base de datos - considerar √≠ndices adicionales")
        
        if self.ui_rendering_score < 85:
            recommendations.append("üé® Optimizar rendering de UI - usar lazy loading para componentes pesados")
        
        if self.memory_efficiency_score < 90:
            recommendations.append("üß† Revisar memory leaks - limpiar referencias no utilizadas")
        
        if self.computed_vars_score < 85:
            recommendations.append("‚ö° Optimizar computed variables - usar cache m√°s agresivo")
        
        # Recomendaciones espec√≠ficas basadas en suite m√°s lenta
        if self.benchmark_suites:
            slowest_suite = min(self.benchmark_suites.values(), key=lambda s: s.average_performance_score)
            if slowest_suite.average_performance_score < 80:
                recommendations.append(f"üéØ Priorizar optimizaci√≥n de: {slowest_suite.suite_name}")
        
        if not recommendations:
            recommendations.append("‚úÖ Performance excelente - mantener buenas pr√°cticas actuales")
        
        return recommendations
    
    @rx.var(cache=True)
    def performance_trend_analysis(self) -> Dict[str, Any]:
        """An√°lisis de tendencias de performance"""
        if len(self.performance_timeline) < 2:
            return {"trend": "insufficient_data", "change": 0.0, "message": "Necesita m√°s datos"}
        
        # Calcular tendencia de los √∫ltimos resultados
        recent_scores = [score for _, score in self.performance_timeline[-10:]]
        
        if len(recent_scores) >= 2:
            first_half = statistics.mean(recent_scores[:len(recent_scores)//2])
            second_half = statistics.mean(recent_scores[len(recent_scores)//2:])
            change = ((second_half - first_half) / first_half) * 100
            
            if change > 5:
                trend = "improving"
                message = f"üìà Performance mejorando ({change:.1f}%)"
            elif change < -5:
                trend = "degrading"
                message = f"üìâ Performance degrad√°ndose ({abs(change):.1f}%)"
            else:
                trend = "stable"
                message = "üìä Performance estable"
            
            return {"trend": trend, "change": change, "message": message}
        
        return {"trend": "stable", "change": 0.0, "message": "üìä Performance estable"}
    
    # ==========================================
    # üöÄ M√âTODOS PRINCIPALES DE BENCHMARKING
    # ==========================================
    
    async def run_full_benchmark_suite(self):
        """
        üöÄ Ejecutar suite completa de benchmarks
        """
        if self.benchmarking_active:
            return
        
        self.benchmarking_active = True
        self.benchmark_progress = 0.0
        self.completed_suites = 0
        self.current_benchmark_results = []
        
        # Suite de benchmarks a ejecutar
        benchmark_suites = [
            ("ui_rendering", "UI Rendering Performance"),
            ("database_operations", "Database Query Performance"), 
            ("computed_variables", "Computed Variables Performance"),
            ("memory_management", "Memory Usage Analysis"),
            ("user_interactions", "User Interaction Responsiveness"),
            ("system_operations", "System Operations Performance")
        ]
        
        self.total_benchmark_suites = len(benchmark_suites)
        
        try:
            for i, (suite_name, description) in enumerate(benchmark_suites):
                self.current_benchmark = description
                self.benchmark_progress = (i / len(benchmark_suites)) * 100
                
                # Ejecutar suite espec√≠fica
                suite_results = await self._run_benchmark_suite(suite_name)
                
                # Crear suite de resultados
                benchmark_suite = BenchmarkSuite.from_results(suite_name, suite_results)
                self.benchmark_suites[suite_name] = benchmark_suite
                
                # Agregar a historial
                self.benchmark_history.append(benchmark_suite)
                
                self.completed_suites += 1
                
                # Peque√±a pausa para UI
                await asyncio.sleep(0.2)
            
            # Generar an√°lisis final
            await self._generate_performance_analysis()
            
        except Exception as e:
            print(f"‚ùå Error en benchmarking: {e}")
            
        finally:
            self.benchmarking_active = False
            self.benchmark_progress = 100.0
    
    async def _run_benchmark_suite(self, suite_name: str) -> List[BenchmarkResult]:
        """
        üéØ Ejecutar suite espec√≠fica de benchmarks
        """
        results = []
        
        if suite_name == "ui_rendering":
            results.extend(await self._benchmark_ui_rendering())
        elif suite_name == "database_operations":
            results.extend(await self._benchmark_database_operations())
        elif suite_name == "computed_variables":
            results.extend(await self._benchmark_computed_variables())
        elif suite_name == "memory_management":
            results.extend(await self._benchmark_memory_management())
        elif suite_name == "user_interactions":
            results.extend(await self._benchmark_user_interactions())
        elif suite_name == "system_operations":
            results.extend(await self._benchmark_system_operations())
        
        return results
    
    async def _benchmark_ui_rendering(self) -> List[BenchmarkResult]:
        """üé® Benchmark de rendering de UI"""
        results = []
        
        # Simular benchmarks de componentes UI
        ui_operations = [
            "odontograma_svg_render",
            "patient_list_render", 
            "intervention_form_render",
            "dashboard_stats_render",
            "modal_open_close"
        ]
        
        for operation in ui_operations:
            result = await self._execute_single_benchmark(
                operation_name=operation,
                benchmark_type=BenchmarkType.UI_RENDERING,
                benchmark_function=self._simulate_ui_operation
            )
            results.append(result)
        
        return results
    
    async def _benchmark_database_operations(self) -> List[BenchmarkResult]:
        """üóÑÔ∏è Benchmark de operaciones de base de datos"""
        results = []
        
        db_operations = [
            "load_patients_list",
            "load_services_catalog",
            "create_consultation",
            "update_odontogram",
            "complex_stats_query"
        ]
        
        for operation in db_operations:
            result = await self._execute_single_benchmark(
                operation_name=operation,
                benchmark_type=BenchmarkType.DATABASE_QUERY,
                benchmark_function=self._simulate_db_operation
            )
            results.append(result)
        
        return results
    
    async def _benchmark_computed_variables(self) -> List[BenchmarkResult]:
        """‚ö° Benchmark de computed variables"""
        results = []
        
        # Test computed vars del AppState
        app_state = self.get_state(AppState)
        
        computed_vars = [
            ("pacientes_filtrados", lambda: getattr(app_state, 'pacientes_filtrados_display', [])),
            ("consultas_hoy", lambda: getattr(app_state, 'consultas_hoy', [])),
            ("estadisticas_dashboard", lambda: getattr(app_state, 'estadisticas_generales_computed', {})),
            ("servicios_por_categoria", lambda: getattr(app_state, 'servicios_por_categoria', {}))
        ]
        
        for var_name, var_func in computed_vars:
            result = await self._execute_single_benchmark(
                operation_name=f"computed_var_{var_name}",
                benchmark_type=BenchmarkType.COMPUTED_VAR,
                benchmark_function=lambda: var_func()
            )
            results.append(result)
        
        return results
    
    async def _benchmark_memory_management(self) -> List[BenchmarkResult]:
        """üß† Benchmark de gesti√≥n de memoria"""
        results = []
        
        memory_operations = [
            "large_list_creation",
            "model_instantiation_bulk",
            "cache_operations",
            "state_updates_bulk",
            "garbage_collection_test"
        ]
        
        for operation in memory_operations:
            result = await self._execute_single_benchmark(
                operation_name=operation,
                benchmark_type=BenchmarkType.MEMORY_USAGE,
                benchmark_function=self._simulate_memory_operation
            )
            results.append(result)
        
        return results
    
    async def _benchmark_user_interactions(self) -> List[BenchmarkResult]:
        """üëÜ Benchmark de interacciones de usuario"""
        results = []
        
        interaction_operations = [
            "form_field_update",
            "table_sorting",
            "modal_interactions",
            "navigation_changes",
            "search_filtering"
        ]
        
        for operation in interaction_operations:
            result = await self._execute_single_benchmark(
                operation_name=operation,
                benchmark_type=BenchmarkType.USER_INTERACTION,
                benchmark_function=self._simulate_user_interaction
            )
            results.append(result)
        
        return results
    
    async def _benchmark_system_operations(self) -> List[BenchmarkResult]:
        """‚öôÔ∏è Benchmark de operaciones del sistema"""
        results = []
        
        system_operations = [
            "authentication_check",
            "permission_validation",
            "session_management",
            "error_handling",
            "logging_operations"
        ]
        
        for operation in system_operations:
            result = await self._execute_single_benchmark(
                operation_name=operation,
                benchmark_type=BenchmarkType.OPERATION_TIMING,
                benchmark_function=self._simulate_system_operation
            )
            results.append(result)
        
        return results
    
    async def _execute_single_benchmark(self, operation_name: str, benchmark_type: BenchmarkType, benchmark_function: Callable) -> BenchmarkResult:
        """
        üéØ Ejecutar benchmark individual
        """
        benchmark_id = f"{operation_name}_{int(time.time() * 1000)}"
        
        # Medir memoria inicial
        memory_before = self._get_memory_usage()
        
        # Ejecutar operaci√≥n y medir tiempo
        start_time = time.time() * 1000  # ms
        error = None
        
        try:
            if asyncio.iscoroutinefunction(benchmark_function):
                await benchmark_function()
            else:
                benchmark_function()
        except Exception as e:
            error = str(e)
        
        end_time = time.time() * 1000  # ms
        execution_time = end_time - start_time
        
        # Medir memoria final
        memory_after = self._get_memory_usage()
        memory_delta = memory_after - memory_before
        
        # Determinar nivel de performance
        performance_level = self._determine_performance_level(execution_time)
        
        return BenchmarkResult(
            benchmark_id=benchmark_id,
            operation_name=operation_name,
            benchmark_type=benchmark_type,
            execution_time_ms=execution_time,
            memory_before_mb=memory_before,
            memory_after_mb=memory_after,
            memory_delta_mb=memory_delta,
            timestamp=datetime.now(),
            context={"suite": benchmark_type.value},
            performance_level=performance_level,
            error=error
        )
    
    # ==========================================
    # üé≠ SIMULADORES DE OPERACIONES
    # ==========================================
    
    async def _simulate_ui_operation(self):
        """Simular operaci√≥n de UI"""
        # Simular tiempo de rendering variado
        delay = random.uniform(0.05, 0.3)
        await asyncio.sleep(delay)
    
    async def _simulate_db_operation(self):
        """Simular operaci√≥n de BD"""
        # Simular query con latencia variable
        delay = random.uniform(0.1, 0.8)
        await asyncio.sleep(delay)
    
    def _simulate_memory_operation(self):
        """Simular operaci√≥n de memoria"""
        # Crear y limpiar datos para simular uso de memoria
        large_list = [{"data": f"item_{i}"} for i in range(1000)]
        # Simular procesamiento
        processed = [item["data"].upper() for item in large_list]
        return len(processed)
    
    def _simulate_user_interaction(self):
        """Simular interacci√≥n de usuario"""
        # Simular tiempo de respuesta a interacci√≥n
        time.sleep(random.uniform(0.01, 0.15))
    
    def _simulate_system_operation(self):
        """Simular operaci√≥n del sistema"""
        # Simular validaci√≥n o procesamiento
        time.sleep(random.uniform(0.02, 0.1))
    
    # ==========================================
    # üìä AN√ÅLISIS Y UTILIDADES
    # ==========================================
    
    async def _generate_performance_analysis(self):
        """
        üìä Generar an√°lisis completo de performance
        """
        # Calcular scores por categor√≠a
        suite_scores = {}
        for suite_name, suite in self.benchmark_suites.items():
            suite_scores[suite_name] = suite.average_performance_score
        
        # Actualizar scores espec√≠ficos
        self.ui_rendering_score = suite_scores.get("ui_rendering", 90.0)
        self.database_performance_score = suite_scores.get("database_operations", 85.0)
        self.memory_efficiency_score = suite_scores.get("memory_management", 92.0)
        self.computed_vars_score = suite_scores.get("computed_variables", 88.0)
        
        # Calcular score general
        if suite_scores:
            self.overall_performance_score = statistics.mean(suite_scores.values())
        
        # Agregar a timeline
        self.performance_timeline.append((datetime.now(), self.overall_performance_score))
        
        # Mantener solo √∫ltimos 100 puntos
        if len(self.performance_timeline) > 100:
            self.performance_timeline = self.performance_timeline[-100:]
        
        # Detectar regresiones
        await self._detect_performance_regressions()
        
        # Generar alertas
        self._generate_performance_alerts()
        
        print(f"üìä An√°lisis completado - Score general: {self.overall_performance_score:.1f}")
    
    async def _detect_performance_regressions(self):
        """üö® Detectar regresiones de performance"""
        self.regression_alerts = []
        
        if len(self.benchmark_history) < 2:
            return
        
        # Comparar con el benchmark anterior
        current_suites = {suite.suite_name: suite for suite in self.benchmark_history[-1:]}
        previous_suites = {suite.suite_name: suite for suite in self.benchmark_history[-2:-1]}
        
        for suite_name in current_suites:
            if suite_name in previous_suites:
                current_score = current_suites[suite_name].average_performance_score
                previous_score = previous_suites[suite_name].average_performance_score
                
                # Detectar regresi√≥n significativa (>10% degradation)
                if current_score < previous_score * 0.9:
                    regression_msg = f"Regresi√≥n detectada en {suite_name}: {previous_score:.1f} ‚Üí {current_score:.1f}"
                    self.regression_alerts.append(regression_msg)
    
    def _generate_performance_alerts(self):
        """üö® Generar alertas de performance"""
        self.performance_alerts = []
        
        # Alert para score general bajo
        if self.overall_performance_score < 70:
            self.performance_alerts.append({
                "severity": "high",
                "message": f"Score general de performance bajo: {self.overall_performance_score:.1f}%",
                "recommendation": "Revisar optimizaciones cr√≠ticas"
            })
        
        # Alert para operaciones muy lentas
        slow_operations = []
        for suite in self.benchmark_suites.values():
            if suite.slowest_operation and suite.slowest_operation.execution_time_ms > 2000:
                slow_operations.append(f"{suite.slowest_operation.operation_name} ({suite.slowest_operation.execution_time_ms:.1f}ms)")
        
        if slow_operations:
            self.performance_alerts.append({
                "severity": "medium",
                "message": f"Operaciones lentas detectadas: {', '.join(slow_operations[:3])}",
                "recommendation": "Optimizar operaciones cr√≠ticas"
            })
        
        # Alert para memory leaks
        memory_issues = []
        for suite in self.benchmark_suites.values():
            for benchmark in suite.benchmarks:
                if benchmark.memory_delta_mb > 10:  # >10MB leak
                    memory_issues.append(benchmark.operation_name)
        
        if memory_issues:
            self.performance_alerts.append({
                "severity": "medium",
                "message": f"Posibles memory leaks en: {', '.join(set(memory_issues[:3]))}",
                "recommendation": "Revisar gesti√≥n de memoria"
            })
    
    def _determine_performance_level(self, execution_time_ms: float) -> PerformanceLevel:
        """üéØ Determinar nivel de performance"""
        if execution_time_ms <= 100:
            return PerformanceLevel.EXCELLENT
        elif execution_time_ms <= 300:
            return PerformanceLevel.GOOD
        elif execution_time_ms <= 1000:
            return PerformanceLevel.ACCEPTABLE
        elif execution_time_ms <= 3000:
            return PerformanceLevel.SLOW
        else:
            return PerformanceLevel.UNACCEPTABLE
    
    def _get_memory_usage(self) -> float:
        """üìè Obtener uso de memoria (mock)"""
        # En producci√≥n usar√≠a psutil o similar
        import sys
        base_memory = 50.0  # Mock baseline
        variable_memory = len(str(sys.getsizeof(self))) / 1024 / 1024
        return base_memory + variable_memory
    
    # ==========================================
    # üõ†Ô∏è M√âTODOS DE CONTROL
    # ==========================================
    
    def toggle_auto_benchmark(self):
        """üîÑ Toggle auto benchmark"""
        self.auto_benchmark_enabled = not self.auto_benchmark_enabled
        print(f"üîÑ Auto benchmark: {'ON' if self.auto_benchmark_enabled else 'OFF'}")
    
    def toggle_detailed_profiling(self):
        """üîç Toggle profiling detallado"""
        self.detailed_profiling = not self.detailed_profiling
        print(f"üîç Detailed profiling: {'ON' if self.detailed_profiling else 'OFF'}")
    
    def clear_benchmark_history(self):
        """üóëÔ∏è Limpiar historial de benchmarks"""
        self.benchmark_history = []
        self.benchmark_suites = {}
        self.performance_timeline = []
        self.performance_alerts = []
        self.regression_alerts = []
        print("üóëÔ∏è Historial de benchmarks limpiado")
    
    async def benchmark_single_operation(self, operation_name: str):
        """üéØ Benchmark de operaci√≥n individual"""
        result = await self._execute_single_benchmark(
            operation_name=operation_name,
            benchmark_type=BenchmarkType.OPERATION_TIMING,
            benchmark_function=self._simulate_system_operation
        )
        
        self.current_benchmark_results.append(result)
        print(f"üéØ Benchmark individual completado: {operation_name} - {result.execution_time_ms:.1f}ms")


# ==========================================
# üé® COMPONENTE UI DEL PERFORMANCE BENCHMARKER
# ==========================================

def performance_benchmark_dashboard() -> rx.Component:
    """üìä Dashboard principal del benchmarker"""
    return rx.box(
        rx.vstack(
            # Header con controles
            rx.hstack(
                rx.icon("bar_chart", size=24, color="purple.600"),
                rx.text("Performance Benchmarker", weight="bold", size="5"),
                rx.spacer(),
                rx.hstack(
                    rx.button(
                        rx.icon("play", size=16),
                        "Run Benchmark",
                        color_scheme="purple",
                        disabled=EstadoPerformanceBenchmarker.benchmarking_active,
                        on_click=EstadoPerformanceBenchmarker.run_full_benchmark_suite
                    ),
                    rx.switch(
                        checked=EstadoPerformanceBenchmarker.auto_benchmark_enabled,
                        on_change=EstadoPerformanceBenchmarker.toggle_auto_benchmark
                    ),
                    rx.text("Auto", size="3"),
                    spacing="2",
                    align_items="center"
                ),
                width="100%",
                align_items="center"
            ),
            
            # Progress bar durante benchmark
            rx.cond(
                EstadoPerformanceBenchmarker.benchmarking_active,
                rx.vstack(
                    rx.progress(
                        value=EstadoPerformanceBenchmarker.benchmark_progress,
                        width="100%"
                    ),
                    rx.text(
                        EstadoPerformanceBenchmarker.current_benchmark,
                        size="2",
                        color="purple.600"
                    ),
                    spacing="2",
                    width="100%"
                ),
                rx.box()
            ),
            
            # M√©tricas principales en grid
            rx.grid(
                rx.stat(
                    rx.stat_label("Overall Score"),
                    rx.stat_number(f"{EstadoPerformanceBenchmarker.overall_performance_score:.1f}%"),
                    rx.stat_help_text(
                        EstadoPerformanceBenchmarker.performance_trend_analysis["message"]
                    )
                ),
                rx.stat(
                    rx.stat_label("UI Rendering"),
                    rx.stat_number(f"{EstadoPerformanceBenchmarker.ui_rendering_score:.1f}%"),
                    rx.stat_help_text("Component rendering")
                ),
                rx.stat(
                    rx.stat_label("Database"),
                    rx.stat_number(f"{EstadoPerformanceBenchmarker.database_performance_score:.1f}%"),
                    rx.stat_help_text("Query performance")
                ),
                rx.stat(
                    rx.stat_label("Memory"),
                    rx.stat_number(f"{EstadoPerformanceBenchmarker.memory_efficiency_score:.1f}%"),
                    rx.stat_help_text("Memory efficiency")
                ),
                columns="4",
                spacing="4",
                width="100%"
            ),
            
            # Alertas de performance
            rx.cond(
                len(EstadoPerformanceBenchmarker.performance_alerts) > 0,
                rx.box(
                    rx.vstack(
                        rx.heading("Performance Alerts", size="4", color="orange.600"),
                        rx.vstack(
                            rx.foreach(
                                EstadoPerformanceBenchmarker.performance_alerts,
                                lambda alert: rx.box(
                                    rx.hstack(
                                        rx.icon(
                                            rx.match(
                                                alert["severity"],
                                                ("high", "triangle_alert"),
                                                ("medium", "alert_circle"),
                                                "info"
                                            ),
                                            size=16,
                                            color=rx.match(
                                                alert["severity"],
                                                ("high", "red.500"),
                                                ("medium", "orange.500"),
                                                "blue.500"
                                            )
                                        ),
                                        rx.vstack(
                                            rx.text(alert["message"], weight="medium", size="3"),
                                            rx.text(alert["recommendation"], size="2", color="gray.600"),
                                            align_items="start",
                                            spacing="1"
                                        ),
                                        spacing="3",
                                        width="100%",
                                        align_items="start"
                                    ),
                                    padding="3",
                                    border_radius="md",
                                    border="1px solid",
                                    border_color="orange.200",
                                    background="orange.50"
                                )
                            ),
                            spacing="2",
                            width="100%"
                        ),
                        spacing="3",
                        width="100%"
                    ),
                    margin_top="4"
                ),
                rx.box()
            ),
            
            # Recomendaciones de optimizaci√≥n
            rx.box(
                rx.vstack(
                    rx.heading("Optimization Recommendations", size="4"),
                    rx.vstack(
                        rx.foreach(
                            EstadoPerformanceBenchmarker.performance_recommendations,
                            lambda rec: rx.box(
                                rx.text(rec, size="3"),
                                padding="2",
                                border_radius="sm",
                                background="green.50",
                                border="1px solid",
                                border_color="green.200"
                            )
                        ),
                        spacing="2",
                        width="100%"
                    ),
                    spacing="3",
                    width="100%"
                ),
                margin_top="4"
            ),
            
            # Controles adicionales
            rx.hstack(
                rx.button(
                    "Clear History",
                    on_click=EstadoPerformanceBenchmarker.clear_benchmark_history,
                    variant="outline"
                ),
                rx.button(
                    "Detailed Profiling",
                    variant=rx.cond(
                        EstadoPerformanceBenchmarker.detailed_profiling,
                        "solid",
                        "outline"
                    ),
                    on_click=EstadoPerformanceBenchmarker.toggle_detailed_profiling
                ),
                spacing="3",
                margin_top="4"
            ),
            
            spacing="4",
            width="100%"
        ),
        padding="4",
        border_radius="lg", 
        border="1px solid",
        border_color="gray.200",
        background="white",
        width="100%"
    )


def benchmark_results_table() -> rx.Component:
    """üìã Tabla de resultados de benchmark"""
    return rx.box(
        rx.vstack(
            rx.heading("Recent Benchmark Results", size="4"),
            
            rx.scroll_area(
                rx.cond(
                    len(EstadoPerformanceBenchmarker.current_benchmark_results) > 0,
                    rx.table.root(
                        rx.table.header(
                            rx.table.row(
                                rx.table.column_header_cell("Operation"),
                                rx.table.column_header_cell("Time (ms)"),
                                rx.table.column_header_cell("Performance"),
                                rx.table.column_header_cell("Memory Œî"),
                                rx.table.column_header_cell("Score")
                            )
                        ),
                        rx.table.body(
                            rx.foreach(
                                EstadoPerformanceBenchmarker.current_benchmark_results,
                                lambda result: rx.table.row(
                                    rx.table.row_header_cell(result.operation_name),
                                    rx.table.cell(f"{result.execution_time_ms:.1f}"),
                                    rx.table.cell(
                                        rx.badge(
                                            result.performance_level.value,
                                            color_scheme=rx.match(
                                                result.performance_level,
                                                (PerformanceLevel.EXCELLENT, "green"),
                                                (PerformanceLevel.GOOD, "blue"),
                                                (PerformanceLevel.ACCEPTABLE, "yellow"),
                                                (PerformanceLevel.SLOW, "orange"),
                                                "red"
                                            )
                                        )
                                    ),
                                    rx.table.cell(f"{result.memory_delta_mb:.1f}MB"),
                                    rx.table.cell(f"{result.performance_score:.1f}")
                                )
                            )
                        ),
                        width="100%"
                    ),
                    rx.text("No benchmark results yet", color="gray.600")
                ),
                height="300px",
                width="100%"
            ),
            
            spacing="3",
            width="100%"
        ),
        padding="4",
        border_radius="md",
        border="1px solid",
        border_color="gray.200",
        background="white",
        width="100%"
    )


def complete_performance_benchmarker() -> rx.Component:
    """
    üìä PERFORMANCE BENCHMARKER COMPLETO
    
    Sistema integral de benchmarking y an√°lisis de performance
    """
    return rx.vstack(
        performance_benchmark_dashboard(),
        benchmark_results_table(),
        spacing="4",
        width="100%",
        max_width="1400px",
        margin="0 auto",
        padding="4"
    )